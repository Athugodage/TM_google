{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "INFO:pymorphy2.opencorpora_dict.wrapper:Loading dictionaries from C:\\conda\\lib\\site-packages\\pymorphy2_dicts_ru\\data\n",
      "INFO:pymorphy2.opencorpora_dict.wrapper:format: 2.4, revision: 417127, updated: 2020-10-11T15:05:51.070345\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import collections\n",
    "import logging\n",
    "import re\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Марк\\tm\n",
      "C:\\Users\\Марк\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('..')\n",
    "  \n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(tokens):\n",
    "    for w in tokens:\n",
    "        w = re.sub('^[0-9]\\.', '', w)\n",
    "        w = re.sub('[a-zA-Z«»\\.]', '', w)\n",
    "        if len(w) == 1 and not w.isalpha() or w == '...' or w == '``' or w == '\\'\\'' or w.isnumeric() or len(\n",
    "                w) == 0 or w == '':\n",
    "            continue\n",
    "        for sw in w.split('-'):\n",
    "            yield sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "def hyphen_words(tokens):\n",
    "    for w in tokens:\n",
    "#         w = w.encode(\"utf-8\").replace('«', '').decode(\"utf-8\")\n",
    "        for sw in w.split('-'):\n",
    "            yield sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(w):\n",
    "    return (morph.parse(w))[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(words, windowSize=2):\n",
    "    for index, word in enumerate(words):\n",
    "        for otherIndex in range(index - windowSize, index + windowSize + 1):\n",
    "            if otherIndex >= 0 and otherIndex < len(words) and otherIndex != index:\n",
    "                otherWord = words[otherIndex]\n",
    "                p = tuple(sorted([word, otherWord]))\n",
    "                yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_for_bigrams():\n",
    "    with codecs.open(PMI_FILE, 'r', 'utf8') as bigrams_pmis_file:\n",
    "        bigrams_pmis = dict()\n",
    "        for line in bigrams_pmis_file.readlines():\n",
    "            ls = line[:-1].split('\\t')\n",
    "            bigrams_pmis[(ls[0], ls[1])] = float(ls[2])\n",
    "    return bigrams_pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query2titles():\n",
    "    with codecs.open(TITLES_FILE, 'r', 'utf8') as titles_file:\n",
    "        q = titles_file.readline()[7:]\n",
    "        titles = titles_file.readlines()[2:]\n",
    "\n",
    "        yield (q, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нать делать история думать русский важный видеть жить понимать \n",
      "\n",
      "['днк вымереть как кость неандерталец принести\\n', 'древний днк европа средний палеолит (неандертальцы)\\n', 'цивилизация родиться в пещера кот шрёдингер\\n', '() человек разумный и всевсевсе пособие\\n', 'неандерталец какой они быть и почему они не стать\\n', 'база информационный потребность\\n', '– вид в который войти четыре подвид\\n', 'человек из денисов пещера оказаться не сапиенс и\\n', 'ктоть третий в геном человек обнаружить след\\n', 'неандерталец\\n', 'эпигенетический анализ показать продолжительность жизнь\\n', 'кроманьонец википедия\\n', 'неандерталец википедия\\n', 'секвенировать геном неандерталец из чагырский пещера\\n', 'неандерталец какой они быть и почему они не стать\\n', 'неандерталец такой же человек как мы просто они не\\n', '(неандерталец) в свет\\n', 'тот в с мосина южный урал в каменный век\\n', 'чеплыгин владимир николаевич мой всемирный история\\n', 'генетика завершить работа антрополог над сам\\n', 'эволюция человек книга обезьяна кость и ген\\n', 'генетический след ведущий к\\n', 'год в наука предок\\n']\n"
     ]
    }
   ],
   "source": [
    "'''with codecs.open(TITLES_FILE, 'r', 'utf8') as titles_file:\n",
    "    q = titles_file.readline()[7:]\n",
    "    titles = titles_file.readlines()[2:]\n",
    "                \n",
    "    print(q)\n",
    "    print(titles)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Yandex_titles_10_words.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5fc19aa845d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTITLES_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmassive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmassive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmassive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Yandex_titles_10_words.docx'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    with open(TITLES_FILE, 'r', encoding='utf8') as f:\n",
    "        massive = f.read().split('\\n')\n",
    "        q = massive[0][7:]\n",
    "        titles = massive[2:]\n",
    "                \n",
    "    print(q)\n",
    "    print(titles)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uniq_bigrams(query2titles):\n",
    "    bigrams_uniq = set()\n",
    "    for q, titles in query2titles:\n",
    "        for title in titles:\n",
    "            tokens = word_tokenize(title.lower())\n",
    "            words = [w for w in to_words(tokens)]\n",
    "            words = [w for w in map(normalize, words) if w not in STOP_WORDS]\n",
    "            for bigram in find_bigrams(words):\n",
    "                bigram = tuple(sorted(bigram))\n",
    "                bigrams_uniq.add(bigram)\n",
    "    return bigrams_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_titles(titles_src):\n",
    "    raw_titles = []\n",
    "    for title in titles_src:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        raw_words = [w for w in hyphen_words(tokens)]\n",
    "        raw_titles.append(raw_words)\n",
    "    return raw_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph(query, titles_src, bigrams_pmis, mode_number=2):\n",
    "    \"\"\"\n",
    "    There are three modes for graph construction:\n",
    "    1) using PMI (requires an appropriate PMI_FILE);\n",
    "    2) using co-occurrence frequencies;\n",
    "    3) unweighted graph.\n",
    "    Mode number 2 is recommended.\n",
    "    \"\"\"\n",
    "    edgeWeights = collections.defaultdict(lambda: collections.Counter())\n",
    "    titles = []\n",
    "    no_pmis = set()\n",
    "    for title in titles_src:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        words = [w for w in to_words(tokens)]\n",
    "        titles.append(words)\n",
    "        words = [w for w in map(normalize, words) if w not in STOP_WORDS]\n",
    "        for bigram in find_bigrams(words):\n",
    "            bigram = tuple(sorted(bigram))\n",
    "            if mode_number == 2:\n",
    "                edgeWeights[bigram[0]][bigram[1]] += 1\n",
    "            elif mode_number == 3:\n",
    "                edgeWeights[bigram[0]][bigram[1]] = 1\n",
    "            elif mode_number == 1:\n",
    "                pmi = bigrams_pmis.get(bigram)\n",
    "                if pmi is not None and pmi > 5:\n",
    "                    edgeWeights[bigram[0]][bigram[1]] += pmi\n",
    "                else:\n",
    "                    no_pmis.add(bigram)\n",
    "    return edgeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_to_file(uniq_bigrams, path):\n",
    "    with codecs.open('bigrams_from_titles.tsv', 'w', 'utf8') as bigrams_file:\n",
    "        for bigram in uniq_bigrams:\n",
    "            bigrams_file.write('\\t'.join(bigram))\n",
    "            bigrams_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  from get_phrases.ipynb\n",
    "##  should be a seperate class\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def get_phrases(words):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    bigrams = find_ngrams(words, 2)\n",
    "    phrases = []\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        normalised = None\n",
    "        p = [morph.parse(b.lower())[0] for b in bigram]\n",
    "        # N + N2, N + N5\n",
    "        if {'NOUN'} in p[0].tag:\n",
    "            if {'NOUN', 'gent'} in p[1].tag or {'NOUN', 'ablt'} in p[1].tag:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + bigram[1]\n",
    "                phrases.append(normalised)\n",
    "            if {'NOUN', 'ablt'} in p[1].tag:\n",
    "                print(p[0].inflect({'nomn'}).word + ' ' + bigram[1])\n",
    "\n",
    "        # Adj + N\n",
    "        elif {'ADJF'} in p[0].tag and {'NOUN'} in p[1].tag:\n",
    "            if p[0].tag.case == p[1].tag.case and p[0].tag.number == p[1].tag.number and p[0].tag.gender == p[\n",
    "                1].tag.gender:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word\n",
    "                phrases.append(normalised)\n",
    "        # Participle + N\n",
    "        elif {'PRTF'} in p[0].tag and {'NOUN'} in p[1].tag:\n",
    "            if p[0].tag.case == p[1].tag.case and p[0].tag.number == p[1].tag.number and p[0].tag.gender == p[\n",
    "                1].tag.gender:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word\n",
    "        # N1\n",
    "        elif {'NOUN'} in p[0].tag and {'NOUN', 'gent'} not in p[1].tag and 'PREP' not in p[1].tag:\n",
    "            normalised = p[0].normal_form\n",
    "        elif len(p) > 1 and {'NOUN'} in p[1].tag and {'ADJF'} not in p[0].tag:\n",
    "            normalised = p[1].normal_form\n",
    "        # Adv + V\n",
    "        elif {'ADVB'} in p[0].tag and {'VERB'} in p[1].tag:\n",
    "            normalised = p[0].normal_form + ' ' + p[1].normal_form\n",
    "            # Adv + V\n",
    "        elif {'ADJS'} in p[0].tag and {'VERB'} in p[1].tag:\n",
    "            normalised = p[0].normal_form + ' ' + p[1].normal_form\n",
    "        if normalised:\n",
    "            phrases.append(normalised.strip())\n",
    "        # V\n",
    "        if {'VERB'} in p[0].tag:\n",
    "            normalised = p[0].normal_form\n",
    "            phrases.append(normalised.strip())\n",
    "        if {'VERB'} in p[1].tag:\n",
    "            normalised = p[1].normal_form\n",
    "            phrases.append(normalised.strip())\n",
    "\n",
    "    trigrams = find_ngrams(words, 3)\n",
    "    for trigram in trigrams:\n",
    "        p = [morph.parse(t)[0] for t in trigram]\n",
    "        normalised = None\n",
    "        # N + Prep + N\n",
    "        if 'NOUN' in p[0].tag and 'PREP' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + trigram[2]\n",
    "        # Adv + Adj + N\n",
    "        elif 'ADVB' in p[0].tag and 'ADJF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = trigram[0] + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # Adv + Participle + N\n",
    "        elif 'ADVB' in p[0].tag and 'PRTF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = trigram[0] + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # Adj + Adj + N\n",
    "        elif 'ADJF' in p[0].tag and 'ADJF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            if p[0].tag.case == p[2].tag.case == p[2].tag.case and p[0].tag.gender == p[2].tag.gender and p[\n",
    "                0].tag.number == p[2].tag.number:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # N + Conj + N\n",
    "        elif 'NOUN' in p[0].tag and 'CONJ' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + p[2].inflect({'nomn'}).word\n",
    "        # N + gent + N2\n",
    "        elif {'NOUN'} in p[0].tag and {'gent'} in p[1].tag and {'NOUN', 'gent'} in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + trigram[2]\n",
    "        if normalised:\n",
    "            phrases.append(normalised.strip())\n",
    "    additional_phrases = []\n",
    "    for idx, p in enumerate(phrases):\n",
    "        if idx + 1 < len(phrases):\n",
    "            next_word = morph.parse(phrases[idx + 1].split()[0])[0].normal_form\n",
    "            current_word = morph.parse(p.split()[-1])[0].normal_form\n",
    "            if current_word == next_word and not set(phrases[idx + 1]).issubset(set(p)):\n",
    "                new_phrase = p + ' ' + ' '.join(phrases[idx + 1].split()[1:])\n",
    "                additional_phrases.append(new_phrase.strip())\n",
    "    if additional_phrases:\n",
    "        phrases.extend(list(set(additional_phrases)))\n",
    "    phrases = list(set([phrase.strip() for phrase in phrases]))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  from pagerank.ipynb\n",
    "##  should be a seperate Python class\n",
    "\n",
    "\n",
    "def __extractNodes(matrix):\n",
    "    nodes = set()\n",
    "    for colKey in matrix:\n",
    "        nodes.add(colKey)\n",
    "    for rowKey in matrix.T:\n",
    "        nodes.add(rowKey)\n",
    "    return nodes\n",
    "\n",
    "def __makeSquare(matrix, keys, default=0.0):\n",
    "    matrix = matrix.copy()\n",
    "\n",
    "    def insertMissingColumns(matrix):\n",
    "        for key in keys:\n",
    "            if not key in matrix:\n",
    "                matrix[key] = pandas.Series(default, index=matrix.index)\n",
    "        return matrix\n",
    "\n",
    "    matrix = insertMissingColumns(matrix) # insert missing columns\n",
    "    matrix = insertMissingColumns(matrix.T).T # insert missing rows\n",
    "\n",
    "    return matrix.fillna(default)\n",
    "\n",
    "\n",
    "def __ensureRowsPositive(matrix):\n",
    "    matrix = matrix.T\n",
    "    for colKey in matrix:\n",
    "        if matrix[colKey].sum() == 0.0:\n",
    "            matrix[colKey] = pandas.Series(numpy.ones(len(matrix[colKey])), index=matrix.index)\n",
    "    return matrix.T    \n",
    "\n",
    "def __normalizeRows(matrix):\n",
    "    return matrix.div(matrix.sum(axis=1), axis=0)\n",
    "\n",
    "def __euclideanNorm(series):\n",
    "    return math.sqrt(series.dot(series))    \n",
    "\n",
    "def __startState(nodes):\n",
    "    if len(nodes) == 0: raise ValueError(\"There must be at least one node.\")\n",
    "    startProb = 1.0 / float(len(nodes))\n",
    "    return pandas.Series({node : startProb for node in nodes})    \n",
    "\n",
    "def __integrateRandomSurfer(nodes, transitionProbs, rsp):\n",
    "    alpha = 1.0 / float(len(nodes)) * rsp\n",
    "    return transitionProbs.copy().multiply(1.0 - rsp) + alpha\n",
    "\n",
    "\n",
    "def powerIteration(transitionWeights, rsp=0.15, epsilon=0.00001, maxIterations=1000):\n",
    "    # Clerical work:\n",
    "    transitionWeights = pandas.DataFrame(transitionWeights)\n",
    "    nodes = __extractNodes(transitionWeights)\n",
    "    transitionWeights = __makeSquare(transitionWeights, nodes, default=0.0)\n",
    "    transitionWeights = __ensureRowsPositive(transitionWeights)\n",
    "\n",
    "    # Setup:\n",
    "    state = __startState(nodes)\n",
    "    transitionProbs = __normalizeRows(transitionWeights)\n",
    "    transitionProbs = __integrateRandomSurfer(nodes, transitionProbs, rsp)\n",
    "\n",
    "    # Power iteration:\n",
    "    for iteration in range(maxIterations):\n",
    "        oldState = state.copy()\n",
    "        state = state.dot(transitionProbs)\n",
    "        delta = state - oldState\n",
    "        if __euclideanNorm(delta) < epsilon: break\n",
    "\n",
    "    return state    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_find_phrases(query, edgeWeights, raw_titles, mode_letter='A'):\n",
    "    \"\"\"\n",
    "    There are four modes for ranking:\n",
    "    A) simply the sum of the values;\n",
    "    B) normalizing the values (by length);\n",
    "    C) accounting for the topic words;\n",
    "    Mode A is recommended.\n",
    "    \"\"\"\n",
    "    query_words = query.split(' ')\n",
    "    wordProbabilities = powerIteration(edgeWeights, rsp=0.15)\n",
    "    wordProbabilities.sort_values(inplace=True, ascending=False)\n",
    "    wordProbabilities = wordProbabilities.to_dict()\n",
    "    rank2phrase = []\n",
    "    for raw_title in raw_titles:\n",
    "        for phrase in get_phrases(raw_title):\n",
    "            query_words_bonus = 0\n",
    "            if mode_letter == 'C':\n",
    "                query_words_bonus = 1\n",
    "            i = 1\n",
    "            sum = 0\n",
    "            for w in phrase.split():\n",
    "                w = normalize(w)\n",
    "                if w not in STOP_WORDS:\n",
    "                    tr = wordProbabilities.get(w)\n",
    "                    if tr:\n",
    "                        sum += tr\n",
    "                        i += 1\n",
    "                        if mode_letter == 'C' and w in query_words:\n",
    "                            query_words_bonus += 1 / ((query_words.index(w) + 1))\n",
    "            if mode_letter == 'A':\n",
    "                r2t = (sum, phrase)\n",
    "            elif mode_letter == 'B':\n",
    "                r2t = (sum / i, phrase)\n",
    "            elif mode_letter == 'C':\n",
    "                r2t = (sum * query_words_bonus, phrase)\n",
    "            rank2phrase.append(r2t)\n",
    "    rank2phrase = sorted(set(rank2phrase), reverse=True)\n",
    "    return rank2phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ranks(first_n=10):\n",
    "    # pmis = get_pmi_for_bigrams()\n",
    "    pmis = []\n",
    "    for query, titles_src in calculate_query2titles():\n",
    "        logging.info('QUERY: ' + query)\n",
    "        graph = calculate_graph(query, titles_src, pmis)\n",
    "        rank2phrases = calculate_and_find_phrases(query, graph, get_raw_titles(titles_src))\n",
    "        if len(rank2phrases) > first_n:\n",
    "            rank2phrases = rank2phrases[:first_n]\n",
    "        for r2t in rank2phrases:\n",
    "            print(r2t[1] + '\\t' + str(r2t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_ranks(path, first_n=10):\n",
    "    pmis = []\n",
    "    with codecs.open(path, 'w', 'utf8') as output_file:\n",
    "        for query, titles_src in calculate_query2titles():\n",
    "            logging.info('==============================')\n",
    "            logging.info('calculating the query: ' + str(query))\n",
    "            output_file.write('TOPIC: ' + str(query))\n",
    "            graph = calculate_graph(str(query), titles_src, pmis)\n",
    "            rank2phrases = calculate_and_find_phrases(str(query), graph, get_raw_titles(titles_src), 'A')\n",
    "            if len(rank2phrases) > first_n:\n",
    "                rank2phrases = rank2phrases[:first_n]\n",
    "#             result = ', '.join([str(x[1].encode(\"utf-8\")) for x in rank2phrases]).decode(\"utf-8\")\n",
    "            result = ', '.join([str(x[1]) for x in rank2phrases])\n",
    "            logging.info('result: ' + result)\n",
    "            output_file.write('LABELS: ' + result + '\\n\\n\\n')\n",
    "    logging.info('saved to: ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_labels():\n",
    "    pmis = []\n",
    "    top_labels = []\n",
    "    for query, titles_src in calculate_query2titles():\n",
    "        graph = calculate_graph(query, titles_src, pmis)\n",
    "        rank2phrases = calculate_and_find_phrases(query, graph, get_raw_titles(titles_src))\n",
    "        top_labels.append(rank2phrases[0][1])\n",
    "    return top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lias_topics/topic_0.txt',\n",
       " 'lias_topics/topic_1.txt',\n",
       " 'lias_topics/topic_2.txt',\n",
       " 'lias_topics/topic_3.txt',\n",
       " 'lias_topics/topic_4.txt',\n",
       " 'lias_topics/topic_5.txt',\n",
       " 'lias_topics/topic_6.txt',\n",
       " 'lias_topics/topic_7.txt',\n",
       " 'lias_topics/topic_8.txt',\n",
       " 'lias_topics/topic_9.txt',\n",
       " 'lias_topics/topic_10.txt',\n",
       " 'lias_topics/topic_11.txt',\n",
       " 'lias_topics/topic_12.txt',\n",
       " 'lias_topics/topic_13.txt',\n",
       " 'lias_topics/topic_14.txt',\n",
       " 'lias_topics/topic_15.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tm\\pageranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: древний возраст кость геном слой находка поздний след популяция днк сапиенс неандерталец зуб признак генетический\n",
      "\n",
      "INFO:root:result: год в наука, кость и ген, человек из денисов, геном, неандерталец\n",
      "INFO:root:saved to: tm/pageranked/topics_0.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: звезда масса галактика чёрный_дыра вселенная излучение солнечный космический наблюдение астроном солнце звёздный спектр ядро дыра\n",
      "\n",
      "INFO:root:result: программа по астрономия для класс, астрономия для класс, программа по астрономия, курс по астрономия, астрономия\n",
      "INFO:root:saved to: tm/pageranked/topics_1.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: белок ген днк опухоль мутация мышь ткань рак участок вирус бактерия геном фермент белка рецептор\n",
      "\n",
      "INFO:root:result: биология в кнкнярыгин, ген и способ, онкогенетик и эпигенетик, геномик и эпигеномики, генетика для студент\n",
      "INFO:root:saved to: tm/pageranked/topics_2.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: частица энергия физика ядро электрон нейтрино детектор протон измерение масса распад коллаборация событие коллайдер нейтрон\n",
      "\n",
      "INFO:root:result: атлас в церн, физика в интернет, введение в физика, чармоний в распад, частица в эксперимент\n",
      "INFO:root:saved to: tm/pageranked/topics_3.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: вода температура атмосфера океан период порода содержание слой кислород глубина морской углерод состав зона лёд\n",
      "\n",
      "INFO:root:result: океан и криосфера в условие, море и океан, океан и криосфера, материк и океан, география и ландшафт\n",
      "INFO:root:saved to: tm/pageranked/topics_4.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: самец самка поведение мозг нейрон сигнал особь половой муха запах потомство отбор частота популяция различие\n",
      "\n",
      "INFO:root:result: биология в кнкнярыгин, наука и жизнь, эстроген в нейрон, женский гормон, выбор\n",
      "INFO:root:saved to: tm/pageranked/topics_5.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: сверхпроводник сверхпроводимость магнитный_поле мантия температура сверхпроводящий магнитный ядро зона материал вихрь образование критический_температура плёнка поле\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нейробиология половой\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:result: наука и жизнь, магнит и сверхпроводник, российский научный фонд, московский физикотехнический институт, научный фонд\n",
      "INFO:root:saved to: tm/pageranked/topics_6.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: молекула химический реакция вода атом водород синтез свойство соединение ион материал молекулярный концентрация электрон температура\n",
      "\n",
      "INFO:root:result: характеристика и нахождение в природа, химия и биотехнология, задача и решение, урок в класс, нахождение в природа\n",
      "INFO:root:saved to: tm/pageranked/topics_7.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: луна затмение полный фаза аппарат солнце астероид свет солнечный планета орбита наблюдение спутник высота тёмный\n",
      "\n",
      "INFO:root:result: билет по астрономия в школа, астрономия в школа, астрономия в задача, ответ к билет по астрономия, учебник по астрономия для курс\n",
      "INFO:root:saved to: tm/pageranked/topics_8.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: теория открытие наука научный физика поле профессор поздний уравнение физический эйнштейн масса история решение коллега\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "эйнштейн неудачником\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:result: эйнштейн и шрёдингер в поиск, наука и жизнь, история и методология, история и философия, эфир и объединение\n",
      "INFO:root:saved to: tm/pageranked/topics_9.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: ген геном позвоночный тело экспрессия генетический эволюционный признак функция мышца орган конечность hox-ген насекомое крыло\n",
      "\n",
      "INFO:root:result: биология в кнкнярыгин, эволюция и ген, ген и хромосома, октябрь г, задание по раздел\n",
      "INFO:root:saved to: tm/pageranked/topics_10.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: эукариот губка происхождение ветвь древний предок многоклеточный_животное митохондрия личинка нервный_система общий_предок эволюционный членистоногий гребневик морской\n",
      "\n",
      "INFO:root:result: олимпиада по биология, биология, московский государственный университет, наука о жизнь, государственный университет\n",
      "INFO:root:saved to: tm/pageranked/topics_11.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: рост быстрый вероятность информация задача слово оценка зависимость мера фактор средний целое дальнейший параметр среднее\n",
      "\n",
      "INFO:root:result: вероятность и статистика, конкуренция и конкурентоспособность, математик и информатика, сущность и содержание, методология и метод\n",
      "INFO:root:saved to: tm/pageranked/topics_12.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: растение птица популяция рыба хищник численность сообщество насекомое дерево лист особь разнообразие пища мелкий муравей\n",
      "\n",
      "INFO:root:result: судьба в сельва, задача и вопрос, растение и животное, биоразнообразие и экология, флора и фауна\n",
      "INFO:root:saved to: tm/pageranked/topics_13.txt\n",
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: свет квантовый атом энергия движение частота температура волна объект сила поле излучение порядок колебание свойство\n",
      "\n",
      "INFO:root:result: атом и молекула в задача, введение в оптик и спектроскопия, молекула в задача, введение в оптик, оптик и спектроскопия для школьник\n",
      "INFO:root:saved to: tm/pageranked/topics_14.txt\n"
     ]
    }
   ],
   "source": [
    "files = [f'lias_topics/topic_{n}.txt' for n in range(len(os.listdir('lias_topics'))-1)]\n",
    "to_path = 'tm/pageranked/'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # show_ranks(1) \n",
    "    TITLES_FILE = to_path + 'topic_0.txt'\n",
    "    STOP_WORDS = 'stopwords.txt'\n",
    "    # print the top label for each topic\n",
    "    for number, file in enumerate(files):\n",
    "        TITLES_FILE = file  ## the input file\n",
    "        new_file = f'{to_path}topics_{number}.txt'  ## file to which we will save the result\n",
    "        \n",
    "        \n",
    "        with open(new_file, 'w', encoding='utf8') as d:\n",
    "            save_top_ranks(new_file, 5) # save the top n labels to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Марк\\tm\\pageranked\n"
     ]
    }
   ],
   "source": [
    "os.chdir('tm\\pageranked')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "topics_0.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_1.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_10.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_11.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_12.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_13.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_14.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_2.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_3.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_4.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_5.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_6.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_7.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_8.txt\n",
      "\n",
      "\n",
      "\n",
      "topics_9.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!type *.txt > all_topics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
