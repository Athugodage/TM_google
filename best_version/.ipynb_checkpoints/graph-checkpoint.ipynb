{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Марк'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-358334bf08a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mМарк\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtm\u001b[0m  \u001b[1;32mimport\u001b[0m \u001b[0mpagerank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_phrases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Марк'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import collections\n",
    "import logging\n",
    "import re\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'lias_topics'\n",
    "TITLES_FILE = PATH + '/topic_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_PATH = '..//stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(STOP_WORDS_PATH, encoding=\"utf-8\") as stop_file:\n",
    "    stop_text = stop_file.read()\n",
    "STOP_WORDS = set(stop_text.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(tokens):\n",
    "    for w in tokens:\n",
    "        w = re.sub('^[0-9]\\.', '', w)\n",
    "        w = re.sub('[a-zA-Z«»\\.]', '', w)\n",
    "        if len(w) == 1 and not w.isalpha() or w == '...' or w == '``' or w == '\\'\\'' or w.isnumeric() or len(\n",
    "                w) == 0 or w == '':\n",
    "            continue\n",
    "        for sw in w.split('-'):\n",
    "            yield sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "def hyphen_words(tokens):\n",
    "    for w in tokens:\n",
    "#         w = w.encode(\"utf-8\").replace('«', '').decode(\"utf-8\")\n",
    "        for sw in w.split('-'):\n",
    "            yield sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(w):\n",
    "    return (morph.parse(w))[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(words, windowSize=2):\n",
    "    for index, word in enumerate(words):\n",
    "        for otherIndex in range(index - windowSize, index + windowSize + 1):\n",
    "            if otherIndex >= 0 and otherIndex < len(words) and otherIndex != index:\n",
    "                otherWord = words[otherIndex]\n",
    "                p = tuple(sorted([word, otherWord]))\n",
    "                yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_for_bigrams():\n",
    "    with codecs.open(PMI_FILE, 'r', 'utf8') as bigrams_pmis_file:\n",
    "        bigrams_pmis = dict()\n",
    "        for line in bigrams_pmis_file.readlines():\n",
    "            ls = line[:-1].split('\\t')\n",
    "            bigrams_pmis[(ls[0], ls[1])] = float(ls[2])\n",
    "    return bigrams_pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query2titles():\n",
    "    with codecs.open(TITLES_FILE, 'r', 'utf8') as titles_file:\n",
    "        q = None\n",
    "        titles = []\n",
    "        for line in titles_file.readlines():\n",
    "            if line.startswith('#QUERY: '):\n",
    "                if q is not None:\n",
    "                    yield (q, titles)\n",
    "                    titles = []\n",
    "                q = line.split('#QUERY: ')[1]\n",
    "                q = q[:-1]\n",
    "            elif len(line) > 1:\n",
    "                titles.append(line)\n",
    "    yield (q, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uniq_bigrams(query2titles):\n",
    "    bigrams_uniq = set()\n",
    "    for q, titles in query2titles:\n",
    "        for title in titles:\n",
    "            tokens = word_tokenize(title.lower())\n",
    "            words = [w for w in to_words(tokens)]\n",
    "            words = [w for w in map(normalize, words) if w not in STOP_WORDS]\n",
    "            for bigram in find_bigrams(words):\n",
    "                bigram = tuple(sorted(bigram))\n",
    "                bigrams_uniq.add(bigram)\n",
    "    return bigrams_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_titles(titles_src):\n",
    "    raw_titles = []\n",
    "    for title in titles_src:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        raw_words = [w for w in hyphen_words(tokens)]\n",
    "        raw_titles.append(raw_words)\n",
    "    return raw_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph(query, titles_src, bigrams_pmis, mode_number=2):\n",
    "    \"\"\"\n",
    "    There are three modes for graph construction:\n",
    "    1) using PMI (requires an appropriate PMI_FILE);\n",
    "    2) using co-occurrence frequencies;\n",
    "    3) unweighted graph.\n",
    "    Mode number 2 is recommended.\n",
    "    \"\"\"\n",
    "    edgeWeights = collections.defaultdict(lambda: collections.Counter())\n",
    "    titles = []\n",
    "    no_pmis = set()\n",
    "    for title in titles_src:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        words = [w for w in to_words(tokens)]\n",
    "        titles.append(words)\n",
    "        words = [w for w in map(normalize, words) if w not in STOP_WORDS]\n",
    "        for bigram in find_bigrams(words):\n",
    "            bigram = tuple(sorted(bigram))\n",
    "            if mode_number == 2:\n",
    "                edgeWeights[bigram[0]][bigram[1]] += 1\n",
    "            elif mode_number == 3:\n",
    "                edgeWeights[bigram[0]][bigram[1]] = 1\n",
    "            elif mode_number == 1:\n",
    "                pmi = bigrams_pmis.get(bigram)\n",
    "                if pmi is not None and pmi > 5:\n",
    "                    edgeWeights[bigram[0]][bigram[1]] += pmi\n",
    "                else:\n",
    "                    no_pmis.add(bigram)\n",
    "    return edgeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_to_file(uniq_bigrams, path):\n",
    "    with codecs.open('bigrams_from_titles.tsv', 'w', 'utf8') as bigrams_file:\n",
    "        for bigram in uniq_bigrams:\n",
    "            bigrams_file.write('\\t'.join(bigram))\n",
    "            bigrams_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  from get_phrases.ipynb\n",
    "##  should be a seperate class\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def get_phrases(words):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    bigrams = find_ngrams(words, 2)\n",
    "    phrases = []\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        normalised = None\n",
    "        p = [morph.parse(b.lower())[0] for b in bigram]\n",
    "        # N + N2, N + N5\n",
    "        if {'NOUN'} in p[0].tag:\n",
    "            if {'NOUN', 'gent'} in p[1].tag or {'NOUN', 'ablt'} in p[1].tag:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + bigram[1]\n",
    "                phrases.append(normalised)\n",
    "            if {'NOUN', 'ablt'} in p[1].tag:\n",
    "                print(p[0].inflect({'nomn'}).word + ' ' + bigram[1])\n",
    "\n",
    "        # Adj + N\n",
    "        elif {'ADJF'} in p[0].tag and {'NOUN'} in p[1].tag:\n",
    "            if p[0].tag.case == p[1].tag.case and p[0].tag.number == p[1].tag.number and p[0].tag.gender == p[\n",
    "                1].tag.gender:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word\n",
    "                phrases.append(normalised)\n",
    "        # Participle + N\n",
    "        elif {'PRTF'} in p[0].tag and {'NOUN'} in p[1].tag:\n",
    "            if p[0].tag.case == p[1].tag.case and p[0].tag.number == p[1].tag.number and p[0].tag.gender == p[\n",
    "                1].tag.gender:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word\n",
    "        # N1\n",
    "        elif {'NOUN'} in p[0].tag and {'NOUN', 'gent'} not in p[1].tag and 'PREP' not in p[1].tag:\n",
    "            normalised = p[0].normal_form\n",
    "        elif len(p) > 1 and {'NOUN'} in p[1].tag and {'ADJF'} not in p[0].tag:\n",
    "            normalised = p[1].normal_form\n",
    "        # Adv + V\n",
    "        elif {'ADVB'} in p[0].tag and {'VERB'} in p[1].tag:\n",
    "            normalised = p[0].normal_form + ' ' + p[1].normal_form\n",
    "            # Adv + V\n",
    "        elif {'ADJS'} in p[0].tag and {'VERB'} in p[1].tag:\n",
    "            normalised = p[0].normal_form + ' ' + p[1].normal_form\n",
    "        if normalised:\n",
    "            phrases.append(normalised.strip())\n",
    "        # V\n",
    "        if {'VERB'} in p[0].tag:\n",
    "            normalised = p[0].normal_form\n",
    "            phrases.append(normalised.strip())\n",
    "        if {'VERB'} in p[1].tag:\n",
    "            normalised = p[1].normal_form\n",
    "            phrases.append(normalised.strip())\n",
    "\n",
    "    trigrams = find_ngrams(words, 3)\n",
    "    for trigram in trigrams:\n",
    "        p = [morph.parse(t)[0] for t in trigram]\n",
    "        normalised = None\n",
    "        # N + Prep + N\n",
    "        if 'NOUN' in p[0].tag and 'PREP' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + trigram[2]\n",
    "        # Adv + Adj + N\n",
    "        elif 'ADVB' in p[0].tag and 'ADJF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = trigram[0] + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # Adv + Participle + N\n",
    "        elif 'ADVB' in p[0].tag and 'PRTF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = trigram[0] + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # Adj + Adj + N\n",
    "        elif 'ADJF' in p[0].tag and 'ADJF' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            if p[0].tag.case == p[2].tag.case == p[2].tag.case and p[0].tag.gender == p[2].tag.gender and p[\n",
    "                0].tag.number == p[2].tag.number:\n",
    "                normalised = p[0].inflect({'nomn'}).word + ' ' + p[1].inflect({'nomn'}).word + ' ' + p[2].normal_form\n",
    "        # N + Conj + N\n",
    "        elif 'NOUN' in p[0].tag and 'CONJ' in p[1].tag and 'NOUN' in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + p[2].inflect({'nomn'}).word\n",
    "        # N + gent + N2\n",
    "        elif {'NOUN'} in p[0].tag and {'gent'} in p[1].tag and {'NOUN', 'gent'} in p[2].tag:\n",
    "            normalised = p[0].inflect({'nomn'}).word + ' ' + trigram[1] + ' ' + trigram[2]\n",
    "        if normalised:\n",
    "            phrases.append(normalised.strip())\n",
    "    additional_phrases = []\n",
    "    for idx, p in enumerate(phrases):\n",
    "        if idx + 1 < len(phrases):\n",
    "            next_word = morph.parse(phrases[idx + 1].split()[0])[0].normal_form\n",
    "            current_word = morph.parse(p.split()[-1])[0].normal_form\n",
    "            if current_word == next_word and not set(phrases[idx + 1]).issubset(set(p)):\n",
    "                new_phrase = p + ' ' + ' '.join(phrases[idx + 1].split()[1:])\n",
    "                additional_phrases.append(new_phrase.strip())\n",
    "    if additional_phrases:\n",
    "        phrases.extend(list(set(additional_phrases)))\n",
    "    phrases = list(set([phrase.strip() for phrase in phrases]))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  from pagerank.ipynb\n",
    "##  should be a seperate Python class\n",
    "\n",
    "\n",
    "def __extractNodes(matrix):\n",
    "    nodes = set()\n",
    "    for colKey in matrix:\n",
    "        nodes.add(colKey)\n",
    "    for rowKey in matrix.T:\n",
    "        nodes.add(rowKey)\n",
    "    return nodes\n",
    "\n",
    "def __makeSquare(matrix, keys, default=0.0):\n",
    "    matrix = matrix.copy()\n",
    "\n",
    "    def insertMissingColumns(matrix):\n",
    "        for key in keys:\n",
    "            if not key in matrix:\n",
    "                matrix[key] = pandas.Series(default, index=matrix.index)\n",
    "        return matrix\n",
    "\n",
    "    matrix = insertMissingColumns(matrix) # insert missing columns\n",
    "    matrix = insertMissingColumns(matrix.T).T # insert missing rows\n",
    "\n",
    "    return matrix.fillna(default)\n",
    "\n",
    "\n",
    "def __ensureRowsPositive(matrix):\n",
    "    matrix = matrix.T\n",
    "    for colKey in matrix:\n",
    "        if matrix[colKey].sum() == 0.0:\n",
    "            matrix[colKey] = pandas.Series(numpy.ones(len(matrix[colKey])), index=matrix.index)\n",
    "    return matrix.T    \n",
    "\n",
    "def __normalizeRows(matrix):\n",
    "    return matrix.div(matrix.sum(axis=1), axis=0)\n",
    "\n",
    "def __euclideanNorm(series):\n",
    "    return math.sqrt(series.dot(series))    \n",
    "\n",
    "def __startState(nodes):\n",
    "    if len(nodes) == 0: raise ValueError(\"There must be at least one node.\")\n",
    "    startProb = 1.0 / float(len(nodes))\n",
    "    return pandas.Series({node : startProb for node in nodes})    \n",
    "\n",
    "def __integrateRandomSurfer(nodes, transitionProbs, rsp):\n",
    "    alpha = 1.0 / float(len(nodes)) * rsp\n",
    "    return transitionProbs.copy().multiply(1.0 - rsp) + alpha\n",
    "\n",
    "\n",
    "def powerIteration(transitionWeights, rsp=0.15, epsilon=0.00001, maxIterations=1000):\n",
    "    # Clerical work:\n",
    "    transitionWeights = pandas.DataFrame(transitionWeights)\n",
    "    nodes = __extractNodes(transitionWeights)\n",
    "    transitionWeights = __makeSquare(transitionWeights, nodes, default=0.0)\n",
    "    transitionWeights = __ensureRowsPositive(transitionWeights)\n",
    "\n",
    "    # Setup:\n",
    "    state = __startState(nodes)\n",
    "    transitionProbs = __normalizeRows(transitionWeights)\n",
    "    transitionProbs = __integrateRandomSurfer(nodes, transitionProbs, rsp)\n",
    "\n",
    "    # Power iteration:\n",
    "    for iteration in range(maxIterations):\n",
    "        oldState = state.copy()\n",
    "        state = state.dot(transitionProbs)\n",
    "        delta = state - oldState\n",
    "        if __euclideanNorm(delta) < epsilon: break\n",
    "\n",
    "    return state    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_find_phrases(query, edgeWeights, raw_titles, mode_letter='A'):\n",
    "    \"\"\"\n",
    "    There are four modes for ranking:\n",
    "    A) simply the sum of the values;\n",
    "    B) normalizing the values (by length);\n",
    "    C) accounting for the topic words;\n",
    "    Mode A is recommended.\n",
    "    \"\"\"\n",
    "    query_words = query.split(' ')\n",
    "    wordProbabilities = powerIteration(edgeWeights, rsp=0.15)\n",
    "    wordProbabilities.sort_values(inplace=True, ascending=False)\n",
    "    wordProbabilities = wordProbabilities.to_dict()\n",
    "    rank2phrase = []\n",
    "    for raw_title in raw_titles:\n",
    "        for phrase in get_phrases(raw_title):\n",
    "            query_words_bonus = 0\n",
    "            if mode_letter == 'C':\n",
    "                query_words_bonus = 1\n",
    "            i = 1\n",
    "            sum = 0\n",
    "            for w in phrase.split():\n",
    "                w = normalize(w)\n",
    "                if w not in STOP_WORDS:\n",
    "                    tr = wordProbabilities.get(w)\n",
    "                    if tr:\n",
    "                        sum += tr\n",
    "                        i += 1\n",
    "                        if mode_letter == 'C' and w in query_words:\n",
    "                            query_words_bonus += 1 / ((query_words.index(w) + 1))\n",
    "            if mode_letter == 'A':\n",
    "                r2t = (sum, phrase)\n",
    "            elif mode_letter == 'B':\n",
    "                r2t = (sum / i, phrase)\n",
    "            elif mode_letter == 'C':\n",
    "                r2t = (sum * query_words_bonus, phrase)\n",
    "            rank2phrase.append(r2t)\n",
    "    rank2phrase = sorted(set(rank2phrase), reverse=True)\n",
    "    return rank2phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ranks(first_n=10):\n",
    "    # pmis = get_pmi_for_bigrams()\n",
    "    pmis = []\n",
    "    for query, titles_src in calculate_query2titles():\n",
    "        logging.info('QUERY: ' + query)\n",
    "        graph = calculate_graph(query, titles_src, pmis)\n",
    "        rank2phrases = calculate_and_find_phrases(query, graph, get_raw_titles(titles_src))\n",
    "        if len(rank2phrases) > first_n:\n",
    "            rank2phrases = rank2phrases[:first_n]\n",
    "        for r2t in rank2phrases:\n",
    "            print(r2t[1] + '\\t' + str(r2t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_ranks(path, first_n=10):\n",
    "    pmis = []\n",
    "    with codecs.open(path, 'w', 'utf8') as output_file:\n",
    "        for query, titles_src in calculate_query2titles():\n",
    "            logging.info('==============================')\n",
    "            logging.info('calculating the query: ' + str(query))\n",
    "            output_file.write('TOPIC: ' + str(query) + '\\n')\n",
    "            graph = calculate_graph(str(query), titles_src, pmis)\n",
    "            rank2phrases = calculate_and_find_phrases(str(query), graph, get_raw_titles(titles_src), 'A')\n",
    "            if len(rank2phrases) > first_n:\n",
    "                rank2phrases = rank2phrases[:first_n]\n",
    "#             result = ', '.join([str(x[1].encode(\"utf-8\")) for x in rank2phrases]).decode(\"utf-8\")\n",
    "            result = ', '.join([str(x[1]) for x in rank2phrases])\n",
    "            logging.info('result: ' + result)\n",
    "            output_file.write('LABELS: ' + result + '\\n')\n",
    "    logging.info('saved to: ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_labels():\n",
    "    pmis = []\n",
    "    top_labels = []\n",
    "    for query, titles_src in calculate_query2titles():\n",
    "        graph = calculate_graph(query, titles_src, pmis)\n",
    "        rank2phrases = calculate_and_find_phrases(query, graph, get_raw_titles(titles_src))\n",
    "        top_labels.append(rank2phrases[0][1])\n",
    "    return top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==============================\n",
      "INFO:root:calculating the query: None\n",
      "INFO:root:result: программа по астрономия для класс, дыра в вселенная, астрономия для класс, программа по астрономия, курс по астрономия\n",
      "INFO:root:saved to: lias_topics/labels_Yandex_10_words.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # show_ranks(1) \n",
    "    \n",
    "    # print the top label for each topic\n",
    "    save_top_ranks('lias_topics/labels_Yandex_10_words.txt', 5) # save the top n labels to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lias_topics/labels_Yandex_10_words.txt', 'w', encoding='utf8') as f:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
